{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HP\\Desktop\\Projects\\SMS SPAM DETECTION NLP\\nlpvenv\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "c:\\Users\\HP\\Desktop\\Projects\\SMS SPAM DETECTION NLP\\nlpvenv\\Lib\\site-packages\\numpy\\core\\_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8429084380610413\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.19      0.05      0.07       153\n",
      "           1       0.86      0.97      0.91       961\n",
      "\n",
      "    accuracy                           0.84      1114\n",
      "   macro avg       0.53      0.51      0.49      1114\n",
      "weighted avg       0.77      0.84      0.80      1114\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from gensim.utils import simple_preprocess\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load and preprocess the messages\n",
    "messages = pd.read_csv('spam.csv', skiprows=1, names=['Label', 'Message'], usecols=[0, 1], encoding='ISO-8859-1')\n",
    "\n",
    "# Initialize lemmatizer\n",
    "lemma = WordNetLemmatizer()\n",
    "\n",
    "# Preprocess corpus\n",
    "corpus = []\n",
    "for i in range(0, len(messages)):\n",
    "    review = re.sub('[^a-zA-Z]', ' ', messages['Message'][i])  # Remove non-alphabet characters\n",
    "    review = review.lower()  # Convert to lowercase\n",
    "    review = review.split()  # Split the sentence into words\n",
    "    review = [lemma.lemmatize(word) for word in review]  # Lemmatize words\n",
    "    review = ' '.join(review)  # Join words back into a sentence\n",
    "    corpus.append(review)\n",
    "\n",
    "# Tokenize sentences and words\n",
    "words = []\n",
    "sent_tokens = []\n",
    "for sent in corpus:\n",
    "    sent_token = sent_tokenize(sent)\n",
    "    sent_tokens.append(sent_token)\n",
    "    for sent in sent_token:\n",
    "        words.append(simple_preprocess(sent))  # Tokenize and preprocess words\n",
    "\n",
    "# Train Word2Vec model on the words\n",
    "model = gensim.models.Word2Vec(words, vector_size=100, window=5, min_count=1, workers=4)\n",
    "model.save(\"word2vec_model.pkl\")\n",
    "\n",
    "\n",
    "# Define a function to compute the average Word2Vec for a document\n",
    "def avg_word2vec(doc):\n",
    "    word_vectors = np.mean([model.wv[word] for word in doc if word in model.wv.index_to_key],axis=0)\n",
    "    if np.any(np.isnan(word_vectors)):  # Check if there are any valid word vectors\n",
    "        return np.zeros(100)\n",
    "    else:\n",
    "        return word_vectors  # Return a valid vector of size 100 \n",
    "\n",
    "    \n",
    "# Initialize the feature matrix and label list\n",
    "X = []\n",
    "y = []  # To store the labels corresponding to each feature vector\n",
    "\n",
    "# Generate feature vectors and store labels\n",
    "for i in range(len(words)):\n",
    "    feature_vector = avg_word2vec(words[i])\n",
    "    X.append(feature_vector)\n",
    "    y.append(messages['Label'][i])  # Store the corresponding label (ham/spam)\n",
    "\n",
    "# Convert X to a NumPy array\n",
    "X = np.vstack(X)  # Stack the valid feature vectors into a 2D array\n",
    "# Convert y to a NumPy array or a Pandas Series\n",
    "y = np.array(y)\n",
    "# Chnanging ham/spam to 0s and 1s\n",
    "y=pd.get_dummies(y)\n",
    "y=y.iloc[:,0].values\n",
    "y=y.astype(int)\n",
    "\n",
    "## Train Test Split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.20)\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "classifier=RandomForestClassifier()\n",
    "\n",
    "classifier.fit(X_train,y_train)\n",
    "y_pred=classifier.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import accuracy_score,classification_report\n",
    "print(accuracy_score(y_test,y_pred))\n",
    "\n",
    "print(classification_report(y_test,y_pred))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Label\n",
       "ham     4825\n",
       "spam     747\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages['Label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: 'ham,Ok lar... Joking wif u oni' -> Predicted Label: spam\n",
      "Message: 'Nah I don't think he goes to usf, he lives around here though' -> Predicted Label: spam\n",
      "Message: 'Your account balance is low.' -> Predicted Label: spam\n",
      "Message: 'HI How are you' -> Predicted Label: spam\n"
     ]
    }
   ],
   "source": [
    "# Function to preprocess a single message\n",
    "def preprocess_message(message, model):\n",
    "    # Preprocess the text\n",
    "    review = re.sub('[^a-zA-Z]', ' ', message)  # Remove non-alphabet characters\n",
    "    review = review.lower()  # Convert to lowercase\n",
    "    review = review.split()  # Split into words\n",
    "    review = [lemma.lemmatize(word) for word in review]  # Lemmatize words\n",
    "    \n",
    "    # Tokenize and preprocess the sentence\n",
    "    tokens = simple_preprocess(' '.join(review))\n",
    "    \n",
    "    # Compute the average Word2Vec vector\n",
    "    feature_vector = avg_word2vec(tokens)\n",
    "    return feature_vector\n",
    "\n",
    "# Example: Load unseen data\n",
    "unseen_messages = [\"ham,Ok lar... Joking wif u oni\", \"Nah I don't think he goes to usf, he lives around here though\", \"Your account balance is low.\",\"HI How are you\"]\n",
    "\n",
    "# Preprocess and compute feature vectors for unseen data\n",
    "unseen_features = []\n",
    "for message in unseen_messages:\n",
    "    unseen_features.append(preprocess_message(message, model))\n",
    "\n",
    "unseen_features = np.vstack(unseen_features)  # Convert to a 2D NumPy array\n",
    "\n",
    "# Predict labels for unseen data\n",
    "unseen_predictions = classifier.predict(unseen_features)\n",
    "\n",
    "# Map predictions back to 'ham' or 'spam'\n",
    "label_map = {0: \"ham\", 1: \"spam\"}\n",
    "unseen_labels = [label_map[pred] for pred in unseen_predictions]\n",
    "\n",
    "# Print predictions\n",
    "for i, message in enumerate(unseen_messages):\n",
    "    print(f\"Message: '{message}' -> Predicted Label: {unseen_labels[i]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Implementing with xgboost for better performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HP\\Desktop\\Projects\\SMS SPAM DETECTION NLP\\nlpvenv\\Lib\\site-packages\\sklearn\\base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
      "  warnings.warn(\n",
      "c:\\Users\\HP\\Desktop\\Projects\\SMS SPAM DETECTION NLP\\nlpvenv\\Lib\\site-packages\\sklearn\\utils\\_tags.py:354: FutureWarning: The SMOTE or classes from which it inherits use `_get_tags` and `_more_tags`. Please define the `__sklearn_tags__` method, or inherit from `sklearn.base.BaseEstimator` and/or other appropriate mixins such as `sklearn.base.TransformerMixin`, `sklearn.base.ClassifierMixin`, `sklearn.base.RegressorMixin`, and `sklearn.base.OutlierMixin`. From scikit-learn 1.7, not defining `__sklearn_tags__` will raise an error.\n",
      "  warnings.warn(\n",
      "c:\\Users\\HP\\Desktop\\Projects\\SMS SPAM DETECTION NLP\\nlpvenv\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [16:35:52] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0c55ff5f71b100e98-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8900984966303784\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.86      0.88       942\n",
      "           1       0.87      0.92      0.90       987\n",
      "\n",
      "    accuracy                           0.89      1929\n",
      "   macro avg       0.89      0.89      0.89      1929\n",
      "weighted avg       0.89      0.89      0.89      1929\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import numpy as np\n",
    "\n",
    "# Assuming `X` and `y` are prepared\n",
    "\n",
    "# Oversample class 0 using SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the XGBoost model with tuned hyperparameters\n",
    "xgb_classifier = XGBClassifier(\n",
    "    scale_pos_weight=5,  # Manually set for better class 0 handling\n",
    "    max_depth=6,\n",
    "    learning_rate=0.1,\n",
    "    n_estimators=500,\n",
    "    use_label_encoder=False,\n",
    "    eval_metric=\"logloss\",\n",
    "    random_state=42\n",
    ")\n",
    "xgb_classifier.fit(X_train, y_train)\n",
    "xgb_classifier.save_model(\"xgboost_model.json\")\n",
    "\n",
    "\n",
    "# Predictions\n",
    "y_pred = xgb_classifier.predict(X_test)\n",
    "\n",
    "# Evaluate performance\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: 'ham,Ok lar... Joking wif u oni' -> Predicted Label: spam\n",
      "Message: 'Is that seriously how you spell his name' -> Predicted Label: spam\n",
      "Message: 'Nah I don't think he goes to usf, he lives around here though' -> Predicted Label: spam\n",
      "Message: 'Your account balance is low.' -> Predicted Label: spam\n",
      "Message: 'HI How are you' -> Predicted Label: spam\n"
     ]
    }
   ],
   "source": [
    "# Function to preprocess a single message\n",
    "def preprocess_message(message, model):\n",
    "    # Preprocess the text\n",
    "    review = re.sub('[^a-zA-Z]', ' ', message)  # Remove non-alphabet characters\n",
    "    review = review.lower()  # Convert to lowercase\n",
    "    review = review.split()  # Split into words\n",
    "    review = [lemma.lemmatize(word) for word in review]  # Lemmatize words\n",
    "    \n",
    "    # Tokenize and preprocess the sentence\n",
    "    tokens = simple_preprocess(' '.join(review))\n",
    "    \n",
    "    # Compute the average Word2Vec vector\n",
    "    feature_vector = avg_word2vec(tokens)\n",
    "    return feature_vector\n",
    "\n",
    "# Example: Load unseen data\n",
    "unseen_messages = [\"ham,Ok lar... Joking wif u oni\",'Is that seriously how you spell his name', \"Nah I don't think he goes to usf, he lives around here though\", \"Your account balance is low.\",\"HI How are you\"]\n",
    "\n",
    "# Preprocess and compute feature vectors for unseen data\n",
    "unseen_features = []\n",
    "for message in unseen_messages:\n",
    "    unseen_features.append(preprocess_message(message, model))\n",
    "\n",
    "unseen_features = np.vstack(unseen_features)  # Convert to a 2D NumPy array\n",
    "\n",
    "# Predict labels for unseen data\n",
    "unseen_predictions = xgb_classifier.predict(unseen_features)\n",
    "\n",
    "# Map predictions back to 'ham' or 'spam'\n",
    "label_map = {0: \"ham\", 1: \"spam\"}\n",
    "unseen_labels = [label_map[pred] for pred in unseen_predictions]\n",
    "\n",
    "# Print predictions\n",
    "for i, message in enumerate(unseen_messages):\n",
    "    print(f\"Message: '{message}' -> Predicted Label: {unseen_labels[i]}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
